---
title: "Fira: Can We Achieve Full-rank Training of LLMs Under Low-rank Constraint?"
collection: publications
category: preprint
permalink: /publication/2024-10-12-fira
excerpt: 'This paper proposes Fira, a new training framework for Large Language Models that achieves full-rank training performance while maintaining low-rank memory efficiency, outperforming existing approaches in pre-training and fine-tuning experiments.'
date: 2024-10-12
venue: 'arxiv'
paperurl: 'https://arxiv.org/abs/2410.01623'
citation: 'Chen, X., Feng, K., Li, C., Lai, X., Yue, X., Yuan, Y., & Wang, G. (2024). Fira: Can We Achieve Full-rank Training of LLMs Under Low-rank Constraint?. arXiv preprint arXiv:2410.01623.'
---