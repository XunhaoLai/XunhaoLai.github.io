---
permalink: /
title: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

HiðŸ‘‹! My name is Xunhao Lai (èµ–å‹‹è±ª). I am a Masterâ€™s student at the [School of Intelligence Science and Technology](https://sai.pku.edu.cn/znxyenglish/) at [Peking University](https://english.pku.edu.cn), supervised by [Professor Tong Lin](https://www.cis.pku.edu.cn/szdw/zzjs/lt.htm). Before that, I was an undergraduate student at [Yuan Pei College, Peking University](https://yuanpei.pku.edu.cn/en/).

My research focuses on natural language processing and large language models. Specifically, I concentrate on long context models, exploring innovative and efficient attention mechanisms, as well as optimizing the efficiency of model training and inference.

## Publications

<div>
<img src="/images/about_flexprefill.jpg" alt="FlexPrefill" style="float: left; width: 100px; margin-right: 10px;">
**[ICLR 2025 Oral] FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference**

**Xunhao Lai**, Jianqiao Lu, Yao Luo, Yiyuan Ma, Xun Zhou

This paper introduces FlexPrefill, a flexible sparse pre-filling mechanism for large language models that dynamically adjusts attention patterns in real-time, improving speed and accuracy in long-sequence inference compared to prior sparse attention methods.

[arxiv](https://arxiv.org/abs/2502.20766) / [github](https://github.com/bytedance/FlexPrefill)
</div>

<div>
<img src="/images/about_fira.jpg" alt="Fira" style="float: left; width: 100px; margin-right: 10px;">
**Fira: Can We Achieve Full-rank Training of LLMs Under Low-rank Constraint?**

Xi Chen, Kaituo Feng, Changsheng Li, **Xunhao Lai**, Xiangyu Yue, Ye Yuan, Guoren Wang

This paper proposes Fira, a new training framework for Large Language Models that achieves full-rank training performance while maintaining low-rank memory efficiency, outperforming existing approaches in pre-training and fine-tuning experiments.

[arxiv](https://arxiv.org/abs/2410.01623) / [github](https://github.com/xichen-fy/Fira)
</div>

## Open-Source Projects

#### [native-sparse-attention-triton](https://github.com/XunhaoLai/native-sparse-attention-triton)

Implemented the Deepseek Native Sparse Attention kernel using Triton, providing flexible and efficient sparse attention training code.

#### [FlexPrefill](https://github.com/bytedance/FlexPrefill)

Implemented the FlexPrefill long-text inference acceleration algorithm, offering a flexible and efficient acceleration solution for long-text models.

## Contact

E-mail: <laixunhao@pku.edu.cn>

Address: Peking University, Beijing, China
