---
permalink: /
title: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

HiðŸ‘‹! My name is Xunhao Lai (èµ–å‹‹è±ª). I am a Masterâ€™s student at the [School of Intelligence Science and Technology](https://sai.pku.edu.cn/znxyenglish/) at [Peking University](https://english.pku.edu.cn), supervised by [Professor Tong Lin](https://www.cis.pku.edu.cn/szdw/zzjs/lt.htm). Before that, I was an undergraduate student at [Yuan Pei College, Peking University](https://yuanpei.pku.edu.cn/en/).

My research focuses on natural language processing and large language models. Specifically, I concentrate on long context models, exploring innovative and efficient attention mechanisms, as well as optimizing the efficiency of model training and inference.

## Publications

## Publications

### [ICLR 2025 Oral] FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference

**Xunhao Lai**, Jianqiao Lu, Yao Luo, Yiyuan Ma, Xun Zhou

[![arxiv](https://img.shields.io/badge/arXiv-2502.20766-b31b1b.svg?style=flat-square)](https://arxiv.org/abs/2502.20766)
[![GitHub](https://img.shields.io/badge/-GitHub-black?logo=github&style=flat-square)](https://github.com/bytedance/FlexPrefill)

<div style="margin-bottom: 20px;">
<img src="/images/about_flexprefill.jpg" alt="FlexPrefill" style="float: left; width: 33%; margin-right: 10px;">
<p>FlexPrefill is a flexible sparse pre-filling mechanism for LLMs that dynamically adjusts attention patterns in real-time, improving speed and accuracy in long-sequence inference.</p>
</div>

### Fira: Can We Achieve Full-rank Training of LLMs Under Low-rank Constraint?

Xi Chen, Kaituo Feng, Changsheng Li, **Xunhao Lai**, Xiangyu Yue, Ye Yuan, Guoren Wang

[![arxiv](https://img.shields.io/badge/arXiv-2410.01623-b31b1b.svg?style=flat-square)](https://arxiv.org/abs/2410.01623)
[![GitHub](https://img.shields.io/badge/-GitHub-black?logo=github&style=flat-square)](https://github.com/xichen-fy/Fira)

<div style="margin-bottom: 20px;">
<img src="/images/about_fira.jpg" alt="Fira" style="float: left; width: 33%; margin-right: 10px;">
<p>Fira is a new training framework for LLMs that achieves full-rank training performance while maintaining low-rank memory efficiency in both pre-training and fine-tuning.</p>
</div>

## Open-Source Projects

### [native-sparse-attention-triton](https://github.com/XunhaoLai/native-sparse-attention-triton)

Implemented the Deepseek Native Sparse Attention kernel using Triton, providing flexible and efficient sparse attention training code.

### [FlexPrefill](https://github.com/bytedance/FlexPrefill)

Implemented the FlexPrefill long-text inference acceleration algorithm, offering a flexible and efficient acceleration solution for long-text models.

## Contact

E-mail: <laixunhao@pku.edu.cn>

Address: Peking University, Beijing, China
